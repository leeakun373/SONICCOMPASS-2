"""
å¾®ç¼©éªŒè¯å·¥å…· - å¿«é€ŸéªŒè¯åˆ†ç±»æ•ˆæœ
ä» SQLite æ•°æ®åº“æå–ç‰¹å®šå…³é”®è¯çš„æ•°æ®ï¼Œè¿è¡Œåˆ†ç±»é€»è¾‘ï¼Œç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
"""

import sys
import argparse
import time
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List, Dict, Tuple

# ä¿®å¤ Windows ç»ˆç«¯ç¼–ç 
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from data import SoundminerImporter
from core import DataProcessor, VectorEngine, UCSManager
import umap


def query_by_keyword(importer: SoundminerImporter, keyword: str, limit: int = 500) -> List[Dict]:
    """
    ä»æ•°æ®åº“æŸ¥è¯¢åŒ…å«å…³é”®è¯çš„æ•°æ®ï¼ˆä½¿ç”¨åŸå§‹ SQLï¼‰
    
    Args:
        importer: SoundminerImporter å®ä¾‹
        keyword: æœç´¢å…³é”®è¯
        limit: æœ€å¤§è¿”å›æ•°é‡
    
    Returns:
        å…ƒæ•°æ®å­—å…¸åˆ—è¡¨
    """
    importer._connect()
    cursor = importer.conn.cursor()
    
    # ä½¿ç”¨åŸå§‹ SQL æŸ¥è¯¢ï¼šåœ¨ filename, description, keywords ä¸­æœç´¢
    table_name = importer.table_name
    query = f"""
        SELECT * FROM {table_name}
        WHERE 
            filename LIKE ? OR
            description LIKE ? OR
            keywords LIKE ?
        LIMIT ?
    """
    
    keyword_pattern = f"%{keyword}%"
    cursor.execute(query, (keyword_pattern, keyword_pattern, keyword_pattern, limit))
    
    rows = cursor.fetchall()
    all_columns = [desc[0] for desc in cursor.description]
    
    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    results = []
    for row in rows:
        row_dict = dict(row)
        # æ„å»º rich_context_text
        rich_text = importer._build_rich_context_text(row, all_columns)
        row_dict['rich_context_text'] = rich_text
        row_dict['semantic_text'] = rich_text  # å‘åå…¼å®¹
        results.append(row_dict)
    
    return results


def classify_data(processor: DataProcessor, metadata_list: List[Dict]) -> List[Dict]:
    """
    å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»ï¼ˆè§„åˆ™ + AIï¼‰
    
    Args:
        processor: DataProcessor å®ä¾‹
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
    
    Returns:
        åˆ†ç±»åçš„å…ƒæ•°æ®åˆ—è¡¨ï¼ˆåŒ…å« category å’Œ classification_sourceï¼‰
    """
    import re
    classified = []
    
    for meta_dict in metadata_list:
        # è¿è¡Œåˆ†ç±»é€»è¾‘
        category, source = processor._extract_category(meta_dict)
        
        # ç¡®å®šåˆ†ç±»æ¥æºï¼ˆä½¿ç”¨ä¸ _extract_category ç›¸åŒçš„é€»è¾‘ï¼‰
        classification_source = "UNCATEGORIZED"
        if category != "UNCATEGORIZED":
            rich_text = meta_dict.get('rich_context_text', '') or meta_dict.get('semantic_text', '')
            text_lower = rich_text.lower() if rich_text else ""
            
            # æ£€æŸ¥ Level 0: å¼ºè§„åˆ™ï¼ˆä½¿ç”¨æ•´è¯åŒ¹é…ï¼‰
            rule_matched = False
            for keyword, target_id in processor.strong_rules.items():
                keyword_lower = keyword.lower()
                pattern = rf"\b{re.escape(keyword_lower)}\b"
                if re.search(pattern, text_lower):
                    classification_source = "Level 0 (Rule)"
                    rule_matched = True
                    break
            
            if not rule_matched:
                # æ£€æŸ¥ Level 1: æ˜¾å¼ Metadata
                raw_cat = meta_dict.get('category', '').strip()
                if raw_cat and "MISC" not in raw_cat.upper() and raw_cat.upper() != "UNCATEGORIZED":
                    classification_source = "Level 1 (Explicit Metadata)"
                else:
                    # Level 2: AI é¢„æµ‹
                    classification_source = "Level 2 (AI Prediction)"
        else:
            classification_source = "UNCATEGORIZED"
        
        # æ›´æ–°å…ƒæ•°æ®
        meta_dict['category'] = category
        meta_dict['classification_source'] = classification_source
        classified.append(meta_dict)
    
    return classified


def visualize_results(
    metadata_list: List[Dict],
    embeddings: np.ndarray,
    output_path: Path,
    keyword: str,
    processor: DataProcessor
):
    """
    ä½¿ç”¨ matplotlib ç”Ÿæˆæ•£ç‚¹å›¾
    
    Args:
        metadata_list: åˆ†ç±»åçš„å…ƒæ•°æ®åˆ—è¡¨
        embeddings: å‘é‡åµŒå…¥çŸ©é˜µ
        output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„
        keyword: æœç´¢å…³é”®è¯ï¼ˆç”¨äºæ ‡é¢˜ï¼‰
        processor: DataProcessor å®ä¾‹ï¼ˆç”¨äºè·å– UCSManagerï¼‰
    """
    # è®¡ç®— UMAP é™ç»´ï¼ˆ2Dï¼‰
    print(f"[å¯è§†åŒ–] è®¡ç®— UMAP é™ç»´...")
    reducer = umap.UMAP(
        n_components=2,
        n_neighbors=15,
        min_dist=0.1,
        random_state=42
    )
    
    # æå–æ ‡ç­¾ç”¨äºç›‘ç£å­¦ä¹ 
    targets = []
    for meta in metadata_list:
        cat_id = meta.get('category', 'UNCATEGORIZED')
        if processor.ucs_manager:
            main_cat = processor.ucs_manager.get_main_category_by_id(cat_id)
            targets.append(main_cat if main_cat != "UNCATEGORIZED" else None)
        else:
            targets.append(cat_id if cat_id != "UNCATEGORIZED" else None)
    
    # å¦‚æœæœ‰æ ‡ç­¾ï¼Œä½¿ç”¨ç›‘ç£ UMAP
    if any(t is not None for t in targets):
        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        encoded_targets = []
        for t in targets:
            if t is None:
                encoded_targets.append(-1)
            else:
                encoded_targets.append(t)
        
        unique_targets = sorted(set([t for t in encoded_targets if t != -1]))
        if len(unique_targets) > 1:
            le.fit(unique_targets)
            encoded = [le.transform([t])[0] if t != -1 else -1 for t in encoded_targets]
            coordinates = reducer.fit_transform(embeddings, y=encoded)
        else:
            coordinates = reducer.fit_transform(embeddings)
    else:
        coordinates = reducer.fit_transform(embeddings)
    
    # æŒ‰åˆ†ç±»æ¥æºåˆ†ç»„
    categories = {}
    for i, meta in enumerate(metadata_list):
        cat_id = meta.get('category', 'UNCATEGORIZED')
        source = meta.get('classification_source', 'UNCATEGORIZED')
        
        # è·å–ä¸»ç±»åˆ«åç§°ç”¨äºé¢œè‰²
        if processor.ucs_manager:
            main_cat = processor.ucs_manager.get_main_category_by_id(cat_id)
            label = main_cat if main_cat != "UNCATEGORIZED" else cat_id
        else:
            label = cat_id
        
        if label not in categories:
            categories[label] = {
                'coords': [],
                'sources': [],
                'filenames': []
            }
        
        categories[label]['coords'].append(coordinates[i])
        categories[label]['sources'].append(source)
        categories[label]['filenames'].append(meta.get('filename', 'Unknown'))
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    plt.figure(figsize=(16, 12))
    
    # ä¸ºæ¯ä¸ªç±»åˆ«åˆ†é…é¢œè‰²
    color_map = plt.cm.get_cmap('tab20')
    colors = {cat: color_map(i / len(categories)) for i, cat in enumerate(categories.keys())}
    
    for label, data in categories.items():
        coords = np.array(data['coords'])
        plt.scatter(
            coords[:, 0],
            coords[:, 1],
            label=label,
            alpha=0.6,
            s=50,
            c=[colors[label]]
        )
    
    plt.title(f'åˆ†ç±»éªŒè¯ç»“æœ - å…³é”®è¯: "{keyword}"\nå…± {len(metadata_list)} æ¡æ•°æ®', fontsize=14, fontweight='bold')
    plt.xlabel('UMAP ç»´åº¦ 1', fontsize=12)
    plt.ylabel('UMAP ç»´åº¦ 2', fontsize=12)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"[å¯è§†åŒ–] å›¾ç‰‡å·²ä¿å­˜: {output_path}")
    plt.close()


def print_classification_report(metadata_list: List[Dict]):
    """
    æ‰“å°åˆ†ç±»æŠ¥å‘Š
    
    Args:
        metadata_list: åˆ†ç±»åçš„å…ƒæ•°æ®åˆ—è¡¨
    """
    print("\n" + "="*80)
    print("åˆ†ç±»æŠ¥å‘Š")
    print("="*80)
    
    # ç»Ÿè®¡åˆ†ç±»æ¥æº
    source_counts = {}
    category_counts = {}
    
    for meta in metadata_list:
        source = meta.get('classification_source', 'UNCATEGORIZED')
        cat_id = meta.get('category', 'UNCATEGORIZED')
        
        source_counts[source] = source_counts.get(source, 0) + 1
        category_counts[cat_id] = category_counts.get(cat_id, 0) + 1
    
    print(f"\nğŸ“Š åˆ†ç±»æ¥æºç»Ÿè®¡:")
    for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):
        percentage = count / len(metadata_list) * 100
        print(f"  {source}: {count} ({percentage:.1f}%)")
    
    print(f"\nğŸ“‹ ç±»åˆ«åˆ†å¸ƒ (Top 10):")
    for cat_id, count in sorted(category_counts.items(), key=lambda x: -x[1])[:10]:
        percentage = count / len(metadata_list) * 100
        print(f"  {cat_id}: {count} ({percentage:.1f}%)")
    
    print(f"\nğŸ“ è¯¦ç»†åˆ†ç±»ç»“æœ (å‰20æ¡):")
    print("-" * 80)
    for i, meta in enumerate(metadata_list[:20]):
        filename = meta.get('filename', 'Unknown')
        cat_id = meta.get('category', 'UNCATEGORIZED')
        source = meta.get('classification_source', 'UNCATEGORIZED')
        print(f"{i+1:3d}. {filename[:50]:<50} -> {cat_id:<15} [{source}]")
    
    if len(metadata_list) > 20:
        print(f"\n... è¿˜æœ‰ {len(metadata_list) - 20} æ¡æ•°æ®æœªæ˜¾ç¤º")
    
    print("="*80)


def main():
    parser = argparse.ArgumentParser(description='å¾®ç¼©éªŒè¯å·¥å…· - å¿«é€ŸéªŒè¯åˆ†ç±»æ•ˆæœ')
    parser.add_argument('keyword', type=str, help='æœç´¢å…³é”®è¯ï¼ˆå¦‚ AIR, WEAPON, VEHICLEï¼‰')
    parser.add_argument('--limit', type=int, default=500, help='æœ€å¤§è¿”å›æ•°é‡ï¼ˆé»˜è®¤ 500ï¼‰')
    parser.add_argument('--db', type=str, default='./test_assets/Sonic.sqlite', help='æ•°æ®åº“è·¯å¾„')
    parser.add_argument('--output', type=str, default=None, help='è¾“å‡ºå›¾ç‰‡è·¯å¾„ï¼ˆé»˜è®¤ verification_result.pngï¼‰')
    
    args = parser.parse_args()
    
    keyword = args.keyword.upper()
    db_path = Path(args.db)
    limit = args.limit
    
    if not db_path.exists():
        print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        sys.exit(1)
    
    # é»˜è®¤è¾“å‡ºä¸º verification_result.png
    output_path = Path(args.output) if args.output else Path("verification_result.png")
    
    print(f"ğŸ” å¾®ç¼©éªŒè¯å·¥å…·")
    print(f"å…³é”®è¯: {keyword}")
    print(f"æ•°æ®åº“: {db_path}")
    print(f"æœ€å¤§æ•°é‡: {limit}")
    print(f"è¾“å‡º: {output_path}")
    print()
    
    # 1. åˆå§‹åŒ–ç»„ä»¶
    print("[æ­¥éª¤ 1/5] åˆå§‹åŒ–ç»„ä»¶...")
    importer = SoundminerImporter(db_path=str(db_path))
    vector_engine = VectorEngine(model_path="./models/bge-m3")
    ucs_manager = UCSManager()
    processor = DataProcessor(
        importer=importer,
        vector_engine=vector_engine,
        cache_dir="./cache"
    )
    processor.ucs_manager = ucs_manager
    processor._load_platinum_centroids()
    print("âœ… åˆå§‹åŒ–å®Œæˆ")
    
    # 2. æŸ¥è¯¢æ•°æ®
    print(f"\n[æ­¥éª¤ 2/5] æŸ¥è¯¢åŒ…å« '{keyword}' çš„æ•°æ®...")
    start_time = time.time()
    raw_metadata = query_by_keyword(importer, keyword, limit=limit)
    print(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(raw_metadata)} æ¡æ•°æ®ï¼ˆè€—æ—¶ {time.time() - start_time:.2f} ç§’ï¼‰")
    
    if len(raw_metadata) == 0:
        print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®")
        sys.exit(1)
    
    # 3. å‘é‡åŒ–
    print(f"\n[æ­¥éª¤ 3/5] å‘é‡åŒ–æ•°æ®...")
    start_time = time.time()
    texts = [meta.get('rich_context_text', '') or meta.get('semantic_text', '') for meta in raw_metadata]
    embeddings = vector_engine.encode_batch(texts, batch_size=32, normalize_embeddings=True)
    print(f"âœ… å‘é‡åŒ–å®Œæˆï¼ˆè€—æ—¶ {time.time() - start_time:.2f} ç§’ï¼‰")
    
    # 4. åˆ†ç±»
    print(f"\n[æ­¥éª¤ 4/5] è¿è¡Œåˆ†ç±»é€»è¾‘...")
    start_time = time.time()
    classified_metadata = classify_data(processor, raw_metadata)
    print(f"âœ… åˆ†ç±»å®Œæˆï¼ˆè€—æ—¶ {time.time() - start_time:.2f} ç§’ï¼‰")
    
    # 5. å¯è§†åŒ–
    print(f"\n[æ­¥éª¤ 5/5] ç”Ÿæˆå¯è§†åŒ–...")
    visualize_results(classified_metadata, embeddings, output_path, keyword, processor)
    
    # 6. æ‰“å°æŠ¥å‘Š
    print_classification_report(classified_metadata)
    
    print(f"\nâœ… éªŒè¯å®Œæˆï¼")
    print(f"   å›¾ç‰‡å·²ä¿å­˜: {output_path}")


if __name__ == "__main__":
    main()

