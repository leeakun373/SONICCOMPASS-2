"""
å¾®ç¼©éªŒè¯å·¥å…· - å¿«é€ŸéªŒè¯åˆ†ç±»æ•ˆæœ
ä» SQLite æ•°æ®åº“æå–ç‰¹å®šå…³é”®è¯çš„æ•°æ®ï¼Œè¿è¡Œåˆ†ç±»é€»è¾‘ï¼Œç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
"""

import sys
import argparse
import time
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List, Dict, Tuple

# ä¿®å¤ Windows ç»ˆç«¯ç¼–ç 
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from data import SoundminerImporter
from core import DataProcessor, VectorEngine, UCSManager, inject_category_vectors, umap_config
import umap


def query_by_keyword(importer: SoundminerImporter, keyword: str, limit: int = 500) -> List[Dict]:
    """
    ä»æ•°æ®åº“æŸ¥è¯¢åŒ…å«å…³é”®è¯çš„æ•°æ®ï¼ˆä½¿ç”¨åŸå§‹ SQLï¼‰
    
    Args:
        importer: SoundminerImporter å®ä¾‹
        keyword: æœç´¢å…³é”®è¯
        limit: æœ€å¤§è¿”å›æ•°é‡
    
    Returns:
        å…ƒæ•°æ®å­—å…¸åˆ—è¡¨
    """
    importer._connect()
    
    # ã€ä¿®å¤ã€‘ç¡®ä¿è¡¨åå·²æ£€æµ‹
    if importer.table_name is None:
        importer.table_name = importer._detect_table_name()
        importer.field_mapping = importer.FIELD_MAPPINGS.get(importer.table_name, {})
    
    cursor = importer.conn.cursor()
    
    # ã€ä¿®å¤ã€‘å…ˆè·å–è¡¨çš„å®é™…åˆ—åï¼Œæ”¯æŒå¤§å°å†™ä¸æ•æ„ŸæŸ¥è¯¢
    cursor.execute(f"PRAGMA table_info({importer.table_name})")
    table_info = cursor.fetchall()
    column_names = [col[1] for col in table_info]  # col[1] æ˜¯åˆ—å
    
    # æŸ¥æ‰¾å¯èƒ½çš„å­—æ®µåï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
    filename_col = None
    description_col = None
    keywords_col = None
    
    for col in column_names:
        col_lower = col.lower()
        if col_lower == 'filename' and filename_col is None:
            filename_col = col
        if col_lower == 'description' and description_col is None:
            description_col = col
        if col_lower == 'keywords' and keywords_col is None:
            keywords_col = col
    
    # ä½¿ç”¨å®é™…å­—æ®µåæ„å»ºæŸ¥è¯¢
    table_name = importer.table_name
    conditions = []
    params = []
    
    if filename_col:
        conditions.append(f"{filename_col} LIKE ?")
        params.append(f"%{keyword}%")
    if description_col:
        conditions.append(f"{description_col} LIKE ?")
        params.append(f"%{keyword}%")
    if keywords_col:
        conditions.append(f"{keywords_col} LIKE ?")
        params.append(f"%{keyword}%")
    
    if not conditions:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å­—æ®µï¼Œä½¿ç”¨é€šé…ç¬¦æŸ¥è¯¢æ‰€æœ‰åˆ—
        print(f"[WARNING] æœªæ‰¾åˆ° filename/description/keywords å­—æ®µï¼Œä½¿ç”¨é€šé…ç¬¦æŸ¥è¯¢")
        query = f"SELECT * FROM {table_name} LIMIT ?"
        cursor.execute(query, (limit,))
    else:
        query = f"""
            SELECT * FROM {table_name}
            WHERE {' OR '.join(conditions)}
            LIMIT ?
        """
        params.append(limit)
        cursor.execute(query, tuple(params))
    
    rows = cursor.fetchall()
    all_columns = [desc[0] for desc in cursor.description]
    
    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    results = []
    for row in rows:
        row_dict = dict(row)
        
        # ã€ä¿®å¤ã€‘ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å­—æ®µåï¼ˆæ•°æ®åº“å¯èƒ½æ˜¯å¤§å°å†™æ··åˆï¼‰
        # å°†å­—æ®µåç»Ÿä¸€ä¸ºå°å†™ï¼Œæ–¹ä¾¿åç»­è®¿é—®
        row_dict_lower = {}
        for key, value in row_dict.items():
            row_dict_lower[key.lower()] = value
            row_dict_lower[key] = value  # ä¿ç•™åŸå§‹å­—æ®µå
        
        # æ„å»º rich_context_textï¼ˆä½¿ç”¨åŸå§‹ row å’Œ all_columnsï¼‰
        rich_text = importer._build_rich_context_text(row, all_columns)
        row_dict['rich_context_text'] = rich_text
        row_dict['semantic_text'] = rich_text  # å‘åå…¼å®¹
        
        # ã€ä¿®å¤ã€‘ç¡®ä¿ filename å­—æ®µå¯ç”¨ï¼ˆå°è¯•å¤šç§å¯èƒ½çš„å­—æ®µåï¼‰
        if 'filename' not in row_dict or not row_dict.get('filename'):
            # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„å­—æ®µåï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
            for col in all_columns:
                if col.lower() == 'filename':
                    row_dict['filename'] = row_dict.get(col, 'Unknown')
                    break
            else:
                # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå­—æ®µæˆ– 'Unknown'
                row_dict['filename'] = row_dict.get(all_columns[0] if all_columns else 'Unknown', 'Unknown')
        
        # ã€ä¿®å¤ã€‘ç¡®ä¿ category å­—æ®µå¯ç”¨
        if 'category' not in row_dict:
            for col in all_columns:
                if col.lower() == 'category':
                    row_dict['category'] = row_dict.get(col, '')
                    break
            else:
                row_dict['category'] = ''
        
        results.append(row_dict)
    
    return results


def classify_data(processor: DataProcessor, metadata_list: List[Dict]) -> List[Dict]:
    """
    å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»ï¼ˆè§„åˆ™ + AIï¼‰
    
    ã€é‡è¦ã€‘ä½¿ç”¨ä¸æ­£å¼æµç¨‹å®Œå…¨ç›¸åŒçš„åˆ†ç±»é€»è¾‘ï¼ˆ_extract_categoryï¼‰
    ç¡®ä¿æµ‹è¯•ç»“æœä¸æ­£å¼æµç¨‹ä¸€è‡´ã€‚
    
    Args:
        processor: DataProcessor å®ä¾‹
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
    
    Returns:
        åˆ†ç±»åçš„å…ƒæ•°æ®åˆ—è¡¨ï¼ˆåŒ…å« category å’Œ classification_sourceï¼‰
    """
    classified = []
    
    for meta_dict in metadata_list:
        # ã€å…³é”®ã€‘ä½¿ç”¨ä¸æ­£å¼æµç¨‹å®Œå…¨ç›¸åŒçš„åˆ†ç±»é€»è¾‘
        # _extract_category è¿”å› (category, source) å…ƒç»„
        
        # ã€è°ƒè¯•ã€‘æ£€æŸ¥ ucs_manager æ˜¯å¦å¯ç”¨
        if not processor.ucs_manager:
            print(f"[WARNING] processor.ucs_manager ä¸º Noneï¼Œæ— æ³•è¿›è¡ŒçŸ­è·¯é€»è¾‘åŒ¹é…")
        
        result = processor._extract_category(meta_dict)
        
        if result:
            category, source = result
        else:
            category = "UNCATEGORIZED"
            source = "æœªåˆ†ç±»"
        
        # ã€è°ƒè¯•ã€‘å¦‚æœåˆ†ç±»å¤±è´¥ï¼Œæ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆä»…å‰3æ¡ï¼‰
        if category == "UNCATEGORIZED" and len(classified) < 3:
            filename = meta_dict.get('filename', 'Unknown')
            if filename.startswith('ANML'):
                # æµ‹è¯•çŸ­è·¯é€»è¾‘
                if processor.ucs_manager:
                    test_catid = processor.ucs_manager.resolve_category_from_filename(filename)
                    if test_catid:
                        test_validated = processor.ucs_manager.enforce_strict_category(test_catid)
                        print(f"[è°ƒè¯•] æ–‡ä»¶ {filename[:50]}: çŸ­è·¯é€»è¾‘è¿”å› {test_catid} -> {test_validated}")
        
        # æ›´æ–°å…ƒæ•°æ®
        meta_dict['category'] = category
        meta_dict['classification_source'] = source
        
        # ã€æ–°å¢ã€‘è·å–ä¸»ç±»åˆ«ä¿¡æ¯ï¼ˆç”¨äºèšç±»åˆ†æï¼‰
        if processor.ucs_manager and category != "UNCATEGORIZED":
            main_cat = processor.ucs_manager.get_main_category_by_id(category)
            meta_dict['main_category'] = main_cat if main_cat != "UNCATEGORIZED" else category
        else:
            meta_dict['main_category'] = category
        
        classified.append(meta_dict)
    
    return classified


def _generate_lod0_labels(
    categories: Dict[str, Dict],
    coordinates: np.ndarray,
    metadata_list: List[Dict],
    min_cluster_size: int = 5
) -> List[Tuple[str, Tuple[float, float], int]]:
    """
    ç”Ÿæˆ LOD 0 æ ‡ç­¾ï¼ˆä¸»ç±»åˆ«åŒºåŸŸæ ‡æ³¨ï¼‰
    ç±»ä¼¼äºè½¯ä»¶ä¸­çš„è¿é€šåŸŸåˆ†æï¼Œæ‰¾åˆ°åŒä¸€ä¸»ç±»åˆ«çš„èšé›†åŒºåŸŸå¹¶æ ‡æ³¨
    
    Args:
        categories: æŒ‰ä¸»ç±»åˆ«åˆ†ç»„çš„æ•°æ®å­—å…¸
        coordinates: æ‰€æœ‰ç‚¹çš„åæ ‡
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        min_cluster_size: æœ€å°èšç±»å¤§å°ï¼ˆå°‘äºè¿™ä¸ªæ•°é‡çš„åŒºåŸŸä¸æ ‡æ³¨ï¼‰
    
    Returns:
        æ ‡ç­¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (æ ‡ç­¾æ–‡æœ¬, ä¸­å¿ƒåæ ‡(x, y), ç‚¹æ•°é‡)
    """
    from sklearn.cluster import DBSCAN
    
    labels = []
    
    for main_cat, data in categories.items():
        if main_cat == 'UNCATEGORIZED':
            continue
        
        coords = np.array(data['coords'])
        if len(coords) < min_cluster_size:
            continue
        
        # ä½¿ç”¨ DBSCAN æ‰¾åˆ°èšé›†çš„åŒºåŸŸ
        # eps: èšç±»åŠå¾„ï¼ˆæ ¹æ®åæ ‡èŒƒå›´è‡ªé€‚åº”è°ƒæ•´ï¼‰
        coord_range = coordinates.max(axis=0) - coordinates.min(axis=0)
        avg_range = np.mean(coord_range)
        eps = avg_range * 0.1  # ä½¿ç”¨åæ ‡èŒƒå›´çš„10%ä½œä¸ºèšç±»åŠå¾„
        
        clustering = DBSCAN(eps=eps, min_samples=min_cluster_size).fit(coords)
        
        # ä¸ºæ¯ä¸ªèšç±»æ‰¾åˆ°ä¸­å¿ƒå¹¶ç”Ÿæˆæ ‡ç­¾
        unique_labels = set(clustering.labels_)
        unique_labels.discard(-1)  # ç§»é™¤å™ªå£°ç‚¹
        
        for cluster_id in unique_labels:
            cluster_mask = clustering.labels_ == cluster_id
            cluster_coords = coords[cluster_mask]
            
            if len(cluster_coords) >= min_cluster_size:
                # è®¡ç®—èšç±»ä¸­å¿ƒ
                center = cluster_coords.mean(axis=0)
                labels.append((main_cat, (center[0], center[1]), len(cluster_coords)))
    
    # å¦‚æœ DBSCAN æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„èšç±»ï¼Œä½¿ç”¨ç®€å•çš„ä¸­å¿ƒç‚¹æ–¹æ³•
    if len(labels) == 0:
        for main_cat, data in categories.items():
            if main_cat == 'UNCATEGORIZED':
                continue
            
            coords = np.array(data['coords'])
            if len(coords) >= min_cluster_size:
                center = coords.mean(axis=0)
                labels.append((main_cat, (center[0], center[1]), len(coords)))
    
    return labels


def visualize_results(
    metadata_list: List[Dict],
    embeddings: np.ndarray,
    output_path: Path,
    keyword: str,
    processor: DataProcessor,
    show_lod0_labels: bool = True
):
    """
    ä½¿ç”¨ matplotlib ç”Ÿæˆæ•£ç‚¹å›¾ï¼Œæ”¯æŒ LOD 0 æ ‡ç­¾æ ‡æ³¨
    
    ã€UMAP åæ ‡è¯´æ˜ã€‘
    - Xè½´ï¼ˆUMAP ç»´åº¦ 1ï¼‰: é™ç»´åçš„ç¬¬ä¸€ä¸ªç»´åº¦ï¼Œè¡¨ç¤ºæ•°æ®åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„ä½ç½®
    - Yè½´ï¼ˆUMAP ç»´åº¦ 2ï¼‰: é™ç»´åçš„ç¬¬äºŒä¸ªç»´åº¦ï¼Œè¡¨ç¤ºæ•°æ®åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„ä½ç½®
    - åæ ‡èŒƒå›´: é€šå¸¸ä¸º -10 åˆ° 10 ä¹‹é—´ï¼ˆå–å†³äº UMAP å‚æ•°ï¼‰
    - èšç±»æ•ˆæœ: åŒä¸€ä¸»ç±»åˆ«ï¼ˆå¦‚ WEAPONï¼‰çš„æ•°æ®åº”è¯¥åœ¨åæ ‡ä¸Šèšé›†åœ¨ä¸€èµ·
    
    Args:
        metadata_list: åˆ†ç±»åçš„å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå·²åŒ…å« coordinatesï¼‰
        embeddings: å‘é‡åµŒå…¥çŸ©é˜µ
        output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„
        keyword: æœç´¢å…³é”®è¯ï¼ˆç”¨äºæ ‡é¢˜ï¼‰
        processor: DataProcessor å®ä¾‹ï¼ˆç”¨äºè·å– UCSManagerï¼‰
        show_lod0_labels: æ˜¯å¦æ˜¾ç¤º LOD 0 æ ‡ç­¾ï¼ˆä¸»ç±»åˆ«åŒºåŸŸæ ‡æ³¨ï¼‰
    """
    # è®¡ç®— UMAP é™ç»´ï¼ˆ2Dï¼‰
    print(f"[å¯è§†åŒ–] è®¡ç®— UMAP é™ç»´...")
    print(f"[è¯´æ˜] UMAP åæ ‡å«ä¹‰:")
    print(f"  - Xè½´: é™ç»´åçš„ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆè¯­ä¹‰ç©ºé—´ä½ç½®ï¼‰")
    print(f"  - Yè½´: é™ç»´åçš„ç¬¬äºŒä¸ªç»´åº¦ï¼ˆè¯­ä¹‰ç©ºé—´ä½ç½®ï¼‰")
    print(f"  - åŒä¸€ä¸»ç±»åˆ«çš„æ•°æ®åº”è¯¥åœ¨åæ ‡ä¸Šèšé›†ï¼ˆå½¢æˆ'å¤§é™†'ï¼‰")
    if len(metadata_list) > 5000:
        print(f"[æç¤º] æ•°æ®é‡è¾ƒå¤§ï¼ˆ{len(metadata_list)} æ¡ï¼‰ï¼ŒUMAP è®¡ç®—å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    # æå–æ ‡ç­¾ç”¨äºç›‘ç£å­¦ä¹ ï¼ˆä½¿ç”¨ä¸»ç±»åˆ«ï¼‰
    targets = []
    for meta in metadata_list:
        main_cat = meta.get('main_category', 'UNCATEGORIZED')
        targets.append(main_cat if main_cat != "UNCATEGORIZED" else "UNCATEGORIZED")
    
    # ã€è¶…çº§é”šç‚¹ç­–ç•¥ã€‘ä¿å­˜åŸå§‹å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆç”¨äºå‘é‡æ³¨å…¥ï¼‰
    targets_original = targets.copy()  # ä¿å­˜å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œé¿å…None
    
    # ä½¿ç”¨æ›´å¼ºçš„ç›‘ç£å‚æ•°ï¼ˆä¸ä¸»æµç¨‹ä¸€è‡´ï¼‰
    use_supervised = len(metadata_list) > 100 and any(t != "UNCATEGORIZED" and t is not None for t in targets)
    
    # ã€è¶…çº§é”šç‚¹ç­–ç•¥ã€‘å‘é‡æ³¨å…¥ï¼šå°†ä¸»ç±»åˆ«çš„One-Hotå‘é‡æ³¨å…¥åˆ°éŸ³é¢‘embeddingä¸­
    if use_supervised and len(metadata_list) > 50:  # å°æ•°æ®é›†å¯ä»¥è·³è¿‡ï¼Œé¿å…è¿‡åº¦çº¦æŸ
        print(f"[å¯è§†åŒ–] åº”ç”¨è¶…çº§é”šç‚¹ç­–ç•¥ï¼ˆæ•°æ®é‡: {len(metadata_list)}ï¼‰...")
        # ä»ç»Ÿä¸€é…ç½®è·å–æ³¨å…¥å‚æ•°ï¼ˆæ”¯æŒè‡ªé€‚åº”æƒé‡ï¼‰
        injection_params = umap_config.get_injection_params(
            data_size=len(metadata_list),
            use_adaptive=True  # å¯ç”¨è‡ªé€‚åº”ï¼šå°æ•°æ®é›†ç”¨è¾ƒå°æƒé‡
        )
        X_combined, _ = inject_category_vectors(
            embeddings=embeddings,
            target_labels=targets_original,
            audio_weight=injection_params['audio_weight'],
            category_weight=injection_params['category_weight']
        )
        print(f"[å¯è§†åŒ–] å‘é‡æ³¨å…¥å®Œæˆ: {embeddings.shape} -> {X_combined.shape} (æƒé‡: {injection_params['category_weight']})")
        embeddings = X_combined  # ä½¿ç”¨æ··åˆå‘é‡æ›¿ä»£åŸå§‹embeddings
    else:
        print(f"[å¯è§†åŒ–] è·³è¿‡è¶…çº§é”šç‚¹ç­–ç•¥ï¼ˆæ•°æ®é‡: {len(metadata_list)}ï¼‰...")
    
    # ä»ç»Ÿä¸€é…ç½®è·å–UMAPå‚æ•°ï¼ˆæ”¯æŒè‡ªé€‚åº”å’Œåœºæ™¯åˆ¤æ–­ï¼‰
    umap_params = umap_config.get_umap_params(
        data_size=len(metadata_list),
        use_adaptive=True,  # å¯ç”¨è‡ªé€‚åº”ï¼šå°æ•°æ®é›†ä½¿ç”¨è¾ƒå°çš„ min_dist
        is_supervised=use_supervised  # æ ¹æ®æ˜¯å¦ä¸ºç›‘ç£å­¦ä¹ å†³å®šæ˜¯å¦æ·»åŠ ç›‘ç£å‚æ•°
    )
    # æ ¹æ®æ•°æ®é‡è°ƒæ•´n_neighborsï¼ˆé¿å…è¶…å‡ºæ•°æ®é‡ï¼‰
    umap_params['n_neighbors'] = min(umap_params['n_neighbors'], len(embeddings) - 1) if len(embeddings) > 1 else 15
    
    reducer = umap.UMAP(**umap_params)
    
    # å¦‚æœæœ‰æ ‡ç­¾ï¼Œä½¿ç”¨ç›‘ç£ UMAP
    if use_supervised:
        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        encoded_targets = []
        for t in targets_original:
            if t == "UNCATEGORIZED" or t is None:
                encoded_targets.append(-1)
            else:
                encoded_targets.append(t)
        
        unique_targets = sorted(set([t for t in encoded_targets if t != -1]))
        if len(unique_targets) > 1:
            le.fit(unique_targets)
            encoded = [le.transform([t])[0] if t != -1 else -1 for t in encoded_targets]
            encoded = np.array(encoded)
            coordinates = reducer.fit_transform(embeddings, y=encoded)  # embeddingså·²ç»æ˜¯X_combinedï¼ˆå¦‚æœåº”ç”¨äº†è¶…çº§é”šç‚¹ï¼‰
        else:
            coordinates = reducer.fit_transform(embeddings)
    else:
        coordinates = reducer.fit_transform(embeddings)
    
    # ã€æ–°å¢ã€‘ä¿å­˜åæ ‡åˆ° metadata_listï¼ˆç”¨äºåç»­ CSV å¯¼å‡ºï¼‰
    for i, meta in enumerate(metadata_list):
        meta['umap_x'] = float(coordinates[i][0])
        meta['umap_y'] = float(coordinates[i][1])
    
    # æŒ‰ä¸»ç±»åˆ«åˆ†ç»„ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    categories = {}
    for i, meta in enumerate(metadata_list):
        main_cat = meta.get('main_category', 'UNCATEGORIZED')
        cat_id = meta.get('category', 'UNCATEGORIZED')
        source = meta.get('classification_source', 'UNCATEGORIZED')
        
        # ä½¿ç”¨ä¸»ç±»åˆ«ä½œä¸ºæ ‡ç­¾ï¼ˆç”¨äºéªŒè¯èšç±»æ•ˆæœï¼‰
        label = main_cat
        
        if label not in categories:
            categories[label] = {
                'coords': [],
                'sources': [],
                'filenames': [],
                'catids': []
            }
        
        categories[label]['coords'].append(coordinates[i])
        categories[label]['sources'].append(source)
        categories[label]['filenames'].append(meta.get('filename', 'Unknown'))
        categories[label]['catids'].append(cat_id)
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    plt.figure(figsize=(16, 12))
    
    # ä¸ºæ¯ä¸ªä¸»ç±»åˆ«åˆ†é…é¢œè‰²
    try:
        color_map = plt.colormaps.get_cmap('tab20')
    except AttributeError:
        # å…¼å®¹æ—§ç‰ˆæœ¬ matplotlib
        color_map = plt.cm.get_cmap('tab20')
    
    colors = {cat: color_map(i / len(categories)) for i, cat in enumerate(categories.keys())}
    
    for label, data in categories.items():
        coords = np.array(data['coords'])
        plt.scatter(
            coords[:, 0],
            coords[:, 1],
            label=f"{label} ({len(coords)})",
            alpha=0.6,
            s=50,
            c=[colors[label]]
        )
    
    # ã€æ–°å¢ã€‘LOD 0 æ ‡ç­¾æ ‡æ³¨ï¼ˆä¸»ç±»åˆ«åŒºåŸŸæ ‡æ³¨ï¼‰
    if show_lod0_labels:
        print(f"[å¯è§†åŒ–] ç”Ÿæˆ LOD 0 æ ‡ç­¾ï¼ˆä¸»ç±»åˆ«åŒºåŸŸæ ‡æ³¨ï¼‰...")
        lod0_labels = _generate_lod0_labels(categories, coordinates, metadata_list, min_cluster_size=max(5, len(metadata_list) // 100))
        
        for label_text, (x, y), count in lod0_labels:
            # ç»˜åˆ¶ç™½è‰²åŠé€æ˜èƒŒæ™¯ï¼ˆå¢å¼ºå¯è¯»æ€§ï¼‰
            plt.text(
                x, y, label_text,
                fontsize=14,
                fontweight='bold',
                color='white',
                ha='center',
                va='center',
                bbox=dict(
                    boxstyle='round,pad=0.5',
                    facecolor='black',
                    alpha=0.6,
                    edgecolor='white',
                    linewidth=1.5
                ),
                zorder=100  # ç¡®ä¿æ ‡ç­¾åœ¨æœ€ä¸Šå±‚
            )
        
        print(f"[å¯è§†åŒ–] å·²æ ‡æ³¨ {len(lod0_labels)} ä¸ªä¸»ç±»åˆ«åŒºåŸŸ")
    
    plt.title(f'åˆ†ç±»éªŒè¯ç»“æœ - å…³é”®è¯: "{keyword}"\nå…± {len(metadata_list)} æ¡æ•°æ®' + (' (å«LOD0æ ‡ç­¾)' if show_lod0_labels else ''), 
              fontsize=14, fontweight='bold')
    plt.xlabel('UMAP Dimension 1 (X-axis)', fontsize=12)
    plt.ylabel('UMAP Dimension 2 (Y-axis)', fontsize=12)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"[å¯è§†åŒ–] å›¾ç‰‡å·²ä¿å­˜: {output_path}")
    plt.close()


def print_classification_report(metadata_list: List[Dict], processor: DataProcessor):
    """
    æ‰“å°åˆ†ç±»æŠ¥å‘Š
    
    Args:
        metadata_list: åˆ†ç±»åçš„å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå·²åŒ…å« coordinatesï¼‰
        processor: DataProcessor å®ä¾‹ï¼ˆç”¨äºè·å– UCSManagerï¼‰
    """
    print("\n" + "="*80)
    print("åˆ†ç±»æŠ¥å‘Š")
    print("="*80)
    
    # ç»Ÿè®¡åˆ†ç±»æ¥æº
    source_counts = {}
    category_counts = {}
    main_category_counts = {}
    
    for meta in metadata_list:
        source = meta.get('classification_source', 'UNCATEGORIZED')
        cat_id = meta.get('category', 'UNCATEGORIZED')
        main_cat = meta.get('main_category', 'UNCATEGORIZED')
        
        source_counts[source] = source_counts.get(source, 0) + 1
        category_counts[cat_id] = category_counts.get(cat_id, 0) + 1
        main_category_counts[main_cat] = main_category_counts.get(main_cat, 0) + 1
    
    print(f"\nğŸ“Š åˆ†ç±»æ¥æºç»Ÿè®¡:")
    for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):
        percentage = count / len(metadata_list) * 100
        print(f"  {source}: {count} ({percentage:.1f}%)")
    
    print(f"\nğŸ“‹ ä¸»ç±»åˆ«åˆ†å¸ƒ (Top 10):")
    for main_cat, count in sorted(main_category_counts.items(), key=lambda x: -x[1])[:10]:
        percentage = count / len(metadata_list) * 100
        print(f"  {main_cat}: {count} ({percentage:.1f}%)")
    
    print(f"\nğŸ“‹ CatID åˆ†å¸ƒ (Top 10):")
    for cat_id, count in sorted(category_counts.items(), key=lambda x: -x[1])[:10]:
        percentage = count / len(metadata_list) * 100
        print(f"  {cat_id}: {count} ({percentage:.1f}%)")
    
    print(f"\nğŸ“ è¯¦ç»†åˆ†ç±»ç»“æœ (å‰20æ¡):")
    print("-" * 80)
    print(f"{'åºå·':<5} {'æ–‡ä»¶å':<45} {'CatID':<15} {'ä¸»ç±»åˆ«':<15} {'æ¥æº':<25} {'åæ ‡(X,Y)':<20}")
    print("-" * 80)
    for i, meta in enumerate(metadata_list[:20]):
        filename = meta.get('filename') or meta.get('Filename') or meta.get('FILENAME') or 'Unknown'
        cat_id = meta.get('category', 'UNCATEGORIZED')
        main_cat = meta.get('main_category', 'UNCATEGORIZED')
        source = meta.get('classification_source', 'UNCATEGORIZED')
        x = meta.get('umap_x', 0)
        y = meta.get('umap_y', 0)
        print(f"{i+1:3d}. {str(filename)[:43]:<43} {cat_id:<15} {main_cat:<15} {source[:23]:<23} ({x:.2f}, {y:.2f})")
    
    if len(metadata_list) > 20:
        print(f"\n... è¿˜æœ‰ {len(metadata_list) - 20} æ¡æ•°æ®æœªæ˜¾ç¤º")
    
    print("="*80)


def export_to_csv(metadata_list: List[Dict], output_dir: Path, keyword: str, timestamp: str):
    """
    å¯¼å‡ºè¯¦ç»†æ•°æ®åˆ° CSV æ–‡ä»¶
    
    Args:
        metadata_list: åˆ†ç±»åçš„å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå·²åŒ…å« coordinatesï¼‰
        output_dir: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        keyword: æœç´¢å…³é”®è¯ï¼ˆç”¨äºæ–‡ä»¶åï¼‰
        timestamp: æ—¶é—´æˆ³ï¼ˆæ ¼å¼ï¼šMMDDHHmmï¼‰
    """
    import csv
    
    csv_path = output_dir / f"verify_{keyword}_details_{timestamp}.csv"
    
    with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        
        # è¡¨å¤´
        writer.writerow([
            'åºå·', 'æ–‡ä»¶å', 'CatID', 'ä¸»ç±»åˆ«', 'åˆ†ç±»æ¥æº', 
            'UMAP_X', 'UMAP_Y', 'Rich Text (å‰100å­—ç¬¦)'
        ])
        
        # æ•°æ®è¡Œ
        for i, meta in enumerate(metadata_list):
            filename = meta.get('filename') or meta.get('Filename') or meta.get('FILENAME') or 'Unknown'
            cat_id = meta.get('category', 'UNCATEGORIZED')
            main_cat = meta.get('main_category', 'UNCATEGORIZED')
            source = meta.get('classification_source', 'UNCATEGORIZED')
            x = meta.get('umap_x', 0)
            y = meta.get('umap_y', 0)
            rich_text = meta.get('rich_context_text', '')[:100]
            
            writer.writerow([
                i + 1,
                filename,
                cat_id,
                main_cat,
                source,
                f"{x:.4f}",
                f"{y:.4f}",
                rich_text
            ])
    
    print(f"[å¯¼å‡º] CSV æ–‡ä»¶å·²ä¿å­˜: {csv_path}")
    print(f"       å¯ä»¥ç”¨ Excel æ‰“å¼€ï¼ŒæŸ¥çœ‹è¯¦ç»†æ•°æ®å’Œåæ ‡åˆ†å¸ƒ")


def query_all_data(importer: SoundminerImporter, limit: int = 10000) -> List[Dict]:
    """
    ä»æ•°æ®åº“æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ˆå…¨åº“æ¨¡å¼ï¼‰
    
    Args:
        importer: SoundminerImporter å®ä¾‹
        limit: æœ€å¤§è¿”å›æ•°é‡ï¼ˆé¿å…æ•°æ®è¿‡å¤šï¼‰
    
    Returns:
        å…ƒæ•°æ®å­—å…¸åˆ—è¡¨
    """
    importer._connect()
    
    # ã€ä¿®å¤ã€‘ç¡®ä¿è¡¨åå·²æ£€æµ‹
    if importer.table_name is None:
        importer.table_name = importer._detect_table_name()
        importer.field_mapping = importer.FIELD_MAPPINGS.get(importer.table_name, {})
    
    cursor = importer.conn.cursor()
    table_name = importer.table_name
    
    # æŸ¥è¯¢æ‰€æœ‰æ•°æ®
    query = f"SELECT * FROM {table_name} LIMIT ?"
    cursor.execute(query, (limit,))
    
    rows = cursor.fetchall()
    all_columns = [desc[0] for desc in cursor.description]
    
    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    results = []
    for row in rows:
        row_dict = dict(zip(all_columns, row))
        
        # æ„å»º rich_context_text
        rich_text = importer._build_rich_context_text(row, all_columns)
        row_dict['rich_context_text'] = rich_text
        row_dict['semantic_text'] = rich_text
        
        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
        if 'filename' not in row_dict or not row_dict.get('filename'):
            for col in all_columns:
                if col.lower() == 'filename':
                    row_dict['filename'] = row_dict.get(col, 'Unknown')
                    break
            else:
                row_dict['filename'] = row_dict.get(all_columns[0] if all_columns else 'Unknown', 'Unknown')
        
        if 'category' not in row_dict:
            for col in all_columns:
                if col.lower() == 'category':
                    row_dict['category'] = row_dict.get(col, '')
                    break
            else:
                row_dict['category'] = ''
        
        results.append(row_dict)
    
    return results


def main():
    parser = argparse.ArgumentParser(description='å¾®ç¼©éªŒè¯å·¥å…· - å¿«é€ŸéªŒè¯åˆ†ç±»æ•ˆæœ')
    parser.add_argument('keyword', type=str, nargs='?', default=None, help='æœç´¢å…³é”®è¯ï¼ˆå¦‚ AIR, WEAPON, VEHICLEï¼‰ï¼Œå¯é€‰')
    parser.add_argument('--all', '--full-db', action='store_true', dest='full_db', help='å…¨åº“æ¨¡å¼ï¼šå¤„ç†æ•´ä¸ªæ•°æ®åº“ï¼ˆä¸ä½¿ç”¨å…³é”®è¯ï¼‰')
    parser.add_argument('--limit', type=int, default=500, help='æœ€å¤§è¿”å›æ•°é‡ï¼ˆé»˜è®¤ 500ï¼Œå…¨åº“æ¨¡å¼é»˜è®¤ 10000ï¼‰')
    parser.add_argument('--db', type=str, default=None, help='æ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰')
    parser.add_argument('--output', type=str, default=None, help='è¾“å‡ºå›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--no-lod0', action='store_true', dest='no_lod0', help='ç¦ç”¨ LOD 0 æ ‡ç­¾æ ‡æ³¨')
    
    args = parser.parse_args()
    
    # å…¨åº“æ¨¡å¼ï¼šä¸éœ€è¦å…³é”®è¯
    if args.full_db:
        keyword = "ALL"
        limit = max(args.limit, 1000)  # å…¨åº“æ¨¡å¼è‡³å°‘1000æ¡
        print(f"[æ¨¡å¼] å…¨åº“æ¨¡å¼ï¼šå°†å¤„ç†æœ€å¤š {limit} æ¡æ•°æ®")
    elif args.keyword:
        keyword = args.keyword.upper()
        limit = args.limit
    else:
        print("âŒ é”™è¯¯ï¼šè¯·æŒ‡å®šæœç´¢å…³é”®è¯æˆ–ä½¿ç”¨ --all å‚æ•°è¿›è¡Œå…¨åº“æ¨¡å¼")
        parser.print_help()
        sys.exit(1)
    
    # ã€æ–°å¢ã€‘ä»é…ç½®æ–‡ä»¶è¯»å–æ•°æ®åº“è·¯å¾„ï¼ˆå¦‚æœç”¨æˆ·æœªæŒ‡å®šï¼‰
    if args.db:
        db_path = Path(args.db)
    else:
        from data.database_config import get_database_path
        db_path_str = get_database_path()
        db_path = Path(db_path_str)
        print(f"[INFO] ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®åº“è·¯å¾„: {db_path}")
    
    if not db_path.exists():
        print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        sys.exit(1)
    
    # ã€æ–°å¢ã€‘åˆ›å»ºä¸“å±è¾“å‡ºæ–‡ä»¶å¤¹
    output_dir = Path("verify_output")
    output_dir.mkdir(exist_ok=True)
    
    # ã€æ–°å¢ã€‘ç”Ÿæˆæ—¶é—´æˆ³ï¼ˆæ ¼å¼ï¼šMMDDHHmmï¼Œä¾‹å¦‚ï¼š01061223 è¡¨ç¤º 1æœˆ6æ—¥12ç‚¹23åˆ†ï¼‰
    from datetime import datetime
    timestamp = datetime.now().strftime("%m%d%H%M")
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    if args.output:
        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œæå–æ–‡ä»¶åå¹¶æ·»åŠ æ—¶é—´æˆ³
        user_path = Path(args.output)
        # æ·»åŠ æ—¶é—´æˆ³ï¼šåŸæ–‡ä»¶å_æ—¶é—´æˆ³.æ‰©å±•å
        # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ‰©å±•åï¼Œé»˜è®¤ä½¿ç”¨ .png
        if not user_path.suffix:
            output_path = output_dir / f"{user_path.stem}_{timestamp}.png"
        else:
            output_path = output_dir / f"{user_path.stem}_{timestamp}{user_path.suffix}"
    else:
        # é»˜è®¤æ–‡ä»¶åï¼šverify_{keyword}_{timestamp}.png
        output_path = output_dir / f"verify_{keyword}_{timestamp}.png"
    
    print(f"ğŸ” å¾®ç¼©éªŒè¯å·¥å…·")
    if args.full_db:
        print(f"æ¨¡å¼: å…¨åº“æ¨¡å¼")
    else:
        print(f"å…³é”®è¯: {keyword}")
    print(f"æ•°æ®åº“: {db_path}")
    print(f"æœ€å¤§æ•°é‡: {limit}")
    print(f"è¾“å‡ºæ–‡ä»¶å¤¹: {output_dir}/")
    print(f"è¾“å‡ºå›¾ç‰‡: {output_path.name}")
    print(f"æ—¶é—´æˆ³: {timestamp}")
    print(f"LOD 0 æ ‡ç­¾: {'ç¦ç”¨' if args.no_lod0 else 'å¯ç”¨'}")
    print()
    
    # 1. åˆå§‹åŒ–ç»„ä»¶
    print("[æ­¥éª¤ 1/5] åˆå§‹åŒ–ç»„ä»¶...")
    importer = SoundminerImporter(db_path=str(db_path))
    vector_engine = VectorEngine(model_path="./models/bge-m3")
    ucs_manager = UCSManager()
    processor = DataProcessor(
        importer=importer,
        vector_engine=vector_engine,
        cache_dir="./cache"
    )
    processor.ucs_manager = ucs_manager
    ucs_manager.load_all()  # ç¡®ä¿ UCS Manager å·²åŠ è½½
    processor._load_platinum_centroids()
    
    # ã€è°ƒè¯•ã€‘éªŒè¯ UCS Manager æ˜¯å¦æ­£ç¡®åŠ è½½
    if processor.ucs_manager:
        test_catid = processor.ucs_manager.resolve_category_from_filename("ANMLAqua_Test.wav")
        if test_catid:
            validated = processor.ucs_manager.enforce_strict_category(test_catid)
            print(f"[è°ƒè¯•] çŸ­è·¯é€»è¾‘æµ‹è¯•: ANMLAqua -> {validated}")
    
    print("âœ… åˆå§‹åŒ–å®Œæˆ")
    
    # 2. æŸ¥è¯¢æ•°æ®
    if args.full_db:
        print(f"\n[æ­¥éª¤ 2/5] æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ˆå…¨åº“æ¨¡å¼ï¼‰...")
    else:
        print(f"\n[æ­¥éª¤ 2/5] æŸ¥è¯¢åŒ…å« '{keyword}' çš„æ•°æ®...")
    start_time = time.time()
    
    if args.full_db:
        raw_metadata = query_all_data(importer, limit=limit)
    else:
        raw_metadata = query_by_keyword(importer, keyword, limit=limit)
    
    print(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(raw_metadata)} æ¡æ•°æ®ï¼ˆè€—æ—¶ {time.time() - start_time:.2f} ç§’ï¼‰")
    
    if len(raw_metadata) == 0:
        print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®")
        sys.exit(1)
    
    # ã€è°ƒè¯•ã€‘æ˜¾ç¤ºå‰3æ¡æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
    print(f"\n[è°ƒè¯•] å‰3æ¡æ•°æ®ç¤ºä¾‹:")
    for i, meta in enumerate(raw_metadata[:3]):
        filename = meta.get('filename') or meta.get('Filename') or 'Unknown'
        rich_text = meta.get('rich_context_text', '')
        print(f"  {i+1}. Filename: {filename}")
        print(f"     Rich Text (å‰100å­—ç¬¦): {rich_text[:100] if rich_text else '(ç©º)'}")
        print(f"     Category (åŸå§‹): {meta.get('category', '(ç©º)')}")
        print()
    
    # 3. å‘é‡åŒ–
    print(f"\n[æ­¥éª¤ 3/5] å‘é‡åŒ–æ•°æ®...")
    start_time = time.time()
    texts = [meta.get('rich_context_text', '') or meta.get('semantic_text', '') for meta in raw_metadata]
    embeddings = vector_engine.encode_batch(texts, batch_size=32, normalize_embeddings=True)
    print(f"âœ… å‘é‡åŒ–å®Œæˆï¼ˆè€—æ—¶ {time.time() - start_time:.2f} ç§’ï¼‰")
    
    # 4. åˆ†ç±»
    print(f"\n[æ­¥éª¤ 4/5] è¿è¡Œåˆ†ç±»é€»è¾‘...")
    start_time = time.time()
    classified_metadata = classify_data(processor, raw_metadata)
    print(f"âœ… åˆ†ç±»å®Œæˆï¼ˆè€—æ—¶ {time.time() - start_time:.2f} ç§’ï¼‰")
    
    # 5. å¯è§†åŒ–
    print(f"\n[æ­¥éª¤ 5/5] ç”Ÿæˆå¯è§†åŒ–...")
    visualize_results(classified_metadata, embeddings, output_path, keyword, processor, show_lod0_labels=not args.no_lod0)
    
    # 6. æ‰“å°æŠ¥å‘Š
    print_classification_report(classified_metadata, processor)
    
    # 7. å¯¼å‡º CSVï¼ˆè¯¦ç»†æ•°æ®è¡¨ï¼‰
    print(f"\n[æ­¥éª¤ 6/6] å¯¼å‡ºè¯¦ç»†æ•°æ®åˆ° CSV...")
    export_to_csv(classified_metadata, output_dir, keyword, timestamp)
    
    csv_filename = f"verify_{keyword}_details_{timestamp}.csv"
    
    print(f"\nâœ… éªŒè¯å®Œæˆï¼")
    print(f"   è¾“å‡ºæ–‡ä»¶å¤¹: {output_dir}/")
    print(f"   å›¾ç‰‡å·²ä¿å­˜: {output_path.name}")
    print(f"   CSV å·²ä¿å­˜: {csv_filename}")
    print(f"   æ—¶é—´æˆ³: {timestamp}")
    print(f"\nğŸ’¡ æç¤º:")
    print(f"   - æ‰€æœ‰è¾“å‡ºæ–‡ä»¶éƒ½åœ¨ '{output_dir}/' æ–‡ä»¶å¤¹ä¸­")
    print(f"   - æŸ¥çœ‹ CSV æ–‡ä»¶å¯ä»¥äº†è§£æ¯æ¡æ•°æ®çš„è¯¦ç»†åˆ†ç±»ç»“æœ")
    print(f"   - æ£€æŸ¥ UMAP_X å’Œ UMAP_Y åæ ‡ï¼ŒåŒä¸€ä¸»ç±»åˆ«çš„æ•°æ®åº”è¯¥èšé›†åœ¨ä¸€èµ·")
    print(f"   - å¦‚æœåŒä¸€ä¸»ç±»åˆ«çš„æ•°æ®åˆ†æ•£ï¼Œè¯´æ˜èšç±»æ•ˆæœéœ€è¦æ”¹è¿›")


if __name__ == "__main__":
    main()

