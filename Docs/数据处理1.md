# 数据处理工作流说明

## 概述

Sonic Compass 提供了多个数据处理脚本，用于不同的场景和需求。

## 工作流分类

### 1. 完整重建流程 (Full Rebuild)

**脚本**: `rebuild_atlas.py`

**功能**: 
- 重新向量化所有数据（使用GPU加速）
- 计算Category质心（用于AI语义仲裁）
- **计算UCS模式坐标**（定锚群岛策略：固定坐标+局部UMAP）
- **计算Gravity模式坐标**（纯无监督全局UMAP）
- 保存所有缓存文件

**适用场景**:
- 首次运行
- 数据库更新（新增/删除文件）
- 向量模型更新
- 需要重新计算质心

**耗时**: 最长（取决于数据量，通常10-30分钟）

**前提条件**:
- UCS模式需要 `data_config/ucs_coordinates.json`（如不存在，运行 `tools/extract_category_centroids.py`）

**输出文件**:
- `cache/metadata.pkl` - 元数据
- `cache/embeddings.npy` - 向量矩阵
- `cache/coordinates_ucs.npy` - UCS模式坐标（定锚群岛策略）
- `cache/coordinates_gravity.npy` - Gravity模式坐标（全局UMAP）

---

### 2. 仅重新计算UMAP坐标 (UMAP Recalculation Only)

**脚本**: `recalculate_umap.py`

**功能**:
- 加载现有向量缓存（不重新向量化）
- 使用新参数重新计算UMAP坐标
- 支持UCS模式和Gravity模式独立计算

**适用场景**:
- 调整UMAP参数（`UCS_LOCAL_N_NEIGHBORS_*`, `UCS_LOCAL_MIN_DIST`等）
- 快速测试不同参数效果
- 向量数据未变化，只需更新布局
- 调整UCS坐标配置后重新计算

**耗时**: 中等（通常2-5分钟，取决于数据量）

**前提条件**:
- 必须存在 `cache/metadata.pkl` 和 `cache/embeddings.npy`
- UCS模式需要 `data_config/ucs_coordinates.json`

**使用方法**:
```bash
# 仅计算UCS模式
python recalculate_umap.py --mode ucs

# 仅计算Gravity模式
python recalculate_umap.py --mode gravity

# 同时计算两种模式
python recalculate_umap.py --mode both
```

**输出文件**:
- `cache/coordinates_ucs.npy` - UCS模式坐标（如 `--mode ucs` 或 `--mode both`）
- `cache/coordinates_gravity.npy` - Gravity模式坐标（如 `--mode gravity` 或 `--mode both`）

---

### 3. 仅重新向量化 (Vectors Rebuild Only)

**脚本**: `rebuild_vectors_only.py`

**功能**:
- 重新向量化所有数据
- 保留现有UMAP坐标（但会失效）

**适用场景**:
- 向量模型更新
- 需要重新计算质心
- **注意**: 向量化后必须重新计算UMAP坐标

**耗时**: 较长（通常5-20分钟）

**警告**: 
- 向量化完成后，现有UMAP坐标将不再匹配
- 必须随后运行 `recalculate_umap.py`

**输出文件**:
- `cache/metadata.pkl` - 更新的元数据
- `cache/embeddings.npy` - 更新的向量矩阵

---

### 3. UCS坐标配置生成 (UCS Coordinates Generation)

**脚本**: `tools/extract_category_centroids.py`

**功能**:
- 从现有坐标数据提取82个大类的质心和半径
- 使用中位数（Median）计算质心，避免离群点影响
- 使用1.5×IQR过滤离群点，2%-98%分位数范围计算半径
- 生成 `data_config/ucs_coordinates.json` 初稿

**适用场景**:
- 首次运行UCS模式（生成坐标配置）
- 调整大类坐标布局后重新提取
- 数据更新后需要重新计算大类范围

**耗时**: 很短（通常几秒）

**前提条件**:
- 必须存在 `cache/coordinates_ucs.npy` 或 `cache/coordinates.npy`
- 必须存在 `cache/metadata.pkl`

**使用方法**:
```bash
python tools/extract_category_centroids.py
```

**输出文件**:
- `data_config/ucs_coordinates.json` - UCS坐标配置（包含82个大类的x, y, radius, gap_buffer等）

**注意**: 
- 生成的配置是初稿，可以手动微调坐标位置
- `radius` 有最大值限制（300.0），防止过大范围

---

## 工作流选择指南

### 场景1: 首次运行
```bash
# 1. 如果已有旧坐标数据，先提取UCS配置
python tools/extract_category_centroids.py

# 2. 完整重建（包含UCS和Gravity模式）
python rebuild_atlas.py
```

### 场景2: 调整UCS模式参数（已有向量缓存）
```bash
# 仅重新计算UCS模式坐标
python recalculate_umap.py --mode ucs
```

### 场景3: 调整Gravity模式参数（已有向量缓存）
```bash
# 仅重新计算Gravity模式坐标
python recalculate_umap.py --mode gravity
```

### 场景4: 调整UCS坐标布局
```bash
# 1. 重新提取坐标配置（从最新坐标数据）
python tools/extract_category_centroids.py

# 2. （可选）手动编辑 data_config/ucs_coordinates.json

# 3. 重新计算UCS模式坐标
python recalculate_umap.py --mode ucs
```

### 场景5: 数据库更新（新增文件）
```bash
python rebuild_atlas.py  # 完整重建（包含UCS和Gravity模式）
```

### 场景6: 向量模型更新
```bash
python rebuild_vectors_only.py  # 重新向量化
python recalculate_umap.py --mode both  # 重新计算两种模式的坐标
```

### 场景7: 仅测试UMAP参数效果
```bash
python recalculate_umap.py --mode ucs  # 快速迭代UCS模式
# 或
python recalculate_umap.py --mode gravity  # 快速迭代Gravity模式
```

---

## 软件内按钮

在软件UI中，提供了两个按钮：

1. **🔄 Rebuild Atlas (Full)**
   - 对应 `rebuild_atlas.py`
   - 完整重建流程
   - 在后台线程执行，UI不卡死

2. **🔄 Recalc UMAP Only**
   - 对应 `recalculate_umap.py`
   - 仅重新计算坐标
   - 在后台线程执行，UI不卡死

---

## 参数说明

### UCS模式局部UMAP参数（当前设置）

**位置**: `core/umap_config.py`

- `UCS_LOCAL_N_NEIGHBORS_SMALL = 5`: 小类别（5-50个样本）的邻居数量
- `UCS_LOCAL_N_NEIGHBORS_LARGE = 30`: 大类别（>=1000个样本）的邻居数量
- `UCS_LOCAL_MIN_DIST = 0.05`: 局部UMAP最小距离（紧密聚类）

**说明**:
- 每个大类独立运行局部UMAP，互不影响
- 参数根据大类内数据量自适应选择
- 不使用向量注入（数据已按类别分组）

### Gravity模式参数（当前设置）

**位置**: `core/umap_config.py`

- `GRAVITY_N_NEIGHBORS = 15`: 全局UMAP邻居数量（关注局部结构）
- `MIN_DIST = 0.1`: 最小距离（防止点重叠）
- `SPREAD = 1.0`: 扩散参数

**说明**:
- 使用纯无监督全局UMAP
- 不使用向量注入
- 基于声学特征相似度

### UCS坐标配置参数

**位置**: `data_config/ucs_coordinates.json`

- `x, y`: 大类中心坐标（固定位置）
- `radius`: 大类半径（数据分布范围）
- `gap_buffer`: 缓冲间距（防止大类重叠）
- `MAX_RADIUS_LIMIT = 300.0`: 最大半径限制（防止过大范围）

**说明**:
- 82个大类，每个大类有独立的固定坐标
- 可以使用 `tools/extract_category_centroids.py` 从现有数据提取
- 可以手动编辑调整布局

### 自动质心生成

- `rebuild_atlas.py` 会自动检查并生成 Platinum Centroids（如果缓存不存在）
- 质心文件：`cache/platinum_centroids_754.pkl`
- 包含 753 个标准 UCS CatID 定义向量

### 坐标归一化

**UCS模式**:
- 使用固定坐标范围（`ucs_coordinates.json` 中定义）
- 大类内部坐标归一化到 `[-1, 1]` 然后缩放到 `radius` 范围
- 最终坐标范围：0-3000（由固定坐标配置决定）

**Gravity模式**:
- 全局UMAP计算后，归一化到 0-3000 范围
- 保持数据的相对位置关系

---

## 定锚群岛策略（Fixed Archipelago Strategy）

### 策略说明

**核心思想**: UCS模式使用固定坐标+局部UMAP，彻底解决"大陆漂移"问题

**工作流程**:
1. **加载固定坐标配置** (`data_config/ucs_coordinates.json`)
   - 82个大类的固定中心坐标 (x, y)
   - 每个大类的半径 (radius) 和缓冲间距 (gap_buffer)

2. **按主类别分组数据**
   - 根据分类结果（CatID）获取主类别
   - 将数据按主类别分组

3. **对每个大类独立运行局部UMAP**
   - 只使用该大类内的数据
   - 使用 `UCS_LOCAL_N_NEIGHBORS_*` 和 `UCS_LOCAL_MIN_DIST` 参数
   - **不使用向量注入**（数据已经是单一类别）

4. **归一化并平移到固定位置**
   - 局部坐标归一化到 `[-1, 1]`（使用Robust Scaler，2%-98%分位数）
   - 缩放到有效半径（`radius - gap_buffer`）
   - 平移到大类固定中心

5. **保存坐标**
   - 保存到 `cache/coordinates_ucs.npy`

**优势**:
- ✅ **0%漂移**: 大类位置固定，子类永远不会"叛逃"
- ✅ **保留微观结构**: 大类内部仍然保持UMAP的局部结构
- ✅ **性能提升**: 82个小规模UMAP比1个大UMAP更快
- ✅ **增量更新**: 新增文件只需重算对应大类的局部UMAP

**与旧策略的区别**:
- ❌ **不再使用向量注入**: 旧策略通过注入One-Hot向量强制聚类
- ✅ **使用固定坐标**: 新策略通过硬规则确保大类位置固定
- ✅ **局部UMAP**: 每个大类独立计算，互不干扰

---

## 性能对比

| 工作流 | 向量化 | UCS坐标 | Gravity坐标 | 总耗时 | 适用场景 |
|--------|--------|---------|-------------|--------|----------|
| Full Rebuild | ✅ | ✅ | ✅ | 10-30分钟 | 首次/数据库更新 |
| UMAP Only (UCS) | ❌ | ✅ | ❌ | 2-5分钟 | UCS参数调整 |
| UMAP Only (Gravity) | ❌ | ❌ | ✅ | 2-5分钟 | Gravity参数调整 |
| UMAP Only (Both) | ❌ | ✅ | ✅ | 4-10分钟 | 两种模式都调整 |
| Vectors Only | ✅ | ❌ | ❌ | 5-20分钟 | 模型更新 |
| 坐标配置生成 | ❌ | ⚙️ | ❌ | 几秒 | 生成/更新UCS配置 |

---

---

### 4. 微缩验证工具 (Subset Verification)

**脚本**: `tools/verify_subset.py`

**功能**:
- 从数据库查询包含特定关键词的数据（快速提取子集）
- **使用与正式流程完全相同的分类逻辑**（`_extract_category`）
- 生成 UMAP 可视化散点图
- 打印详细的分类报告（分类来源统计、类别分布、坐标信息）
- **导出 CSV 文件**（包含文件名、CatID、主类别、坐标等详细信息）

**适用场景**:
- 更新 `ucs_alias.csv` 后，快速验证分类效果
- 调试分类逻辑，查看特定关键词的分类结果
- 测试新添加的关键词映射是否生效
- **验证聚类效果**：检查同一主类别（如 WEAPON）下的数据是否在坐标上聚集
- 验证 AI 预测的准确率

**耗时**: 很短（通常 10-30 秒，取决于数据量）

**使用方法**:
```bash
# 基本用法
python tools/verify_subset.py AIR

# 指定参数
python tools/verify_subset.py WEAPON --limit 100 --output verify_weapon.png

# 完整参数
python tools/verify_subset.py SCIFI --limit 200 --db ./test_assets/Sonic.sqlite --output verify_scifi.png
```

**参数说明**:
- `keyword`（必需）: 搜索关键词，会在 filename、description、keywords 中搜索
- `--limit`（可选）: 最大返回数量，默认 500
- `--db`（可选）: 数据库路径，默认 `./test_assets/Sonic.sqlite`
- `--output`（可选）: 输出图片路径，默认 `verification_result.png`

**参数说明**:
- `keyword`（必需）: 搜索关键词，会在 filename、description、keywords 中搜索
- `--limit`（可选）: 最大返回数量，默认 500
- `--db`（可选）: 数据库路径，默认从配置文件读取
- `--output`（可选）: 输出图片路径，默认自动生成
- `--mode`（可选）: 坐标计算模式，`ucs` 或 `gravity`，默认 `ucs`
- `--no-lod0`（可选）: 禁用 LOD 0 标签标注

**输出**:
1. **PNG 图片**：UMAP 散点图
   - **UCS模式**: X/Y轴基于固定坐标配置（定锚群岛策略）
   - **Gravity模式**: X/Y轴为UMAP降维后的语义空间位置
   - 不同主类别用不同颜色标记
   - **聚类效果验证**: 同一主类别的数据应该在坐标上聚集（形成"大陆"）
   - 支持LOD 0标签标注（主类别区域标注）

2. **CSV 文件**：`verify_{keyword}_details.csv`
   - 包含所有数据的详细信息
   - 列：序号、文件名、CatID、主类别、分类来源、UMAP_X、UMAP_Y、Rich Text
   - 可以用 Excel 打开，方便查看和排序

3. **控制台报告**：
   - 分类来源统计（Level -1, 0, 1, 2 各有多少）
   - 主类别分布（每个主类别有多少条）
   - CatID 分布（每个 CatID 有多少条）
   - 详细结果（前20条，包含坐标信息）

**与正式流程的关联性**:
- ✅ **使用相同的分类逻辑**：调用 `DataProcessor._extract_category()`，与正式流程完全一致
- ✅ **使用相同的规则文件**：读取 `data_config/rules.json`（由 `generate_rules_json.py` 生成）
- ✅ **使用相同的质心文件**：加载 `cache/platinum_centroids_754.pkl`（用于 AI 预测）
- ✅ **使用相同的 UCS Manager**：验证和规范化逻辑完全一致
- ✅ **使用相同的布局引擎**：`core/layout_engine.py` - `compute_ucs_layout()` 和 `compute_gravity_layout()`
- ✅ **使用相同的坐标配置**：UCS模式使用 `data_config/ucs_coordinates.json`

**测试目的**:
- **验证分类准确性**：检查文件是否正确分类到预期的 CatID
- **验证聚类效果**：检查同一主类别下的数据是否在 UMAP 坐标上聚集
- **验证规则生效**：检查 Level 0（规则）的命中率
- **验证 AI 预测**：检查 Level 2（AI 预测）的准确率

**优势**:
- ✅ 快速验证，无需运行全量构建（10-30 秒 vs 10-30 分钟）
- ✅ 可视化结果，直观查看分类效果和聚类分布
- ✅ 详细报告，了解分类来源分布
- ✅ CSV 导出，方便在 Excel 中分析和排序
- ✅ 与正式流程完全一致，测试结果可靠

---

## 数据维护完整流程

### 场景：更新关键词映射并验证效果

当你更新了 `ucs_alias.csv` 文件后，需要以下步骤来让更改生效并验证效果：

#### 步骤 1: 标准化 CSV 文件（可选但推荐）

```bash
python tools/standardize_alias_csv.py
```

**作用**:
- 验证 CSV 格式是否正确
- 验证所有 CatID 是否有效
- 规范化 CatID 格式（保持官方 UCS 格式）

**输出**:
- 标准化的 `data_config/ucs_alias.csv`
- 备份文件：`data_config/ucs_alias.csv.backup`
- 警告信息：列出所有无效的 CatID（带行号）

---

#### 步骤 2: 生成规则文件

```bash
python tools/generate_rules_json.py
```

**作用**:
- 从 `ucs_alias.csv` 读取关键词映射
- 归一化关键词（保留空格）
- 验证 CatID 有效性
- 按关键词长度降序排序
- 生成 `data_config/rules.json`

**输出**:
- `data_config/rules.json` - 供 Level 0（强规则）使用
- 警告信息：列出所有无效的 CatID

**重要**: 
- ⚠️ 更新 CSV 后，**必须**运行此脚本重新生成 `rules.json`
- 否则分类系统仍会使用旧的规则

---

#### 步骤 3: 更新质心定义（可选）

如果你修改了 `data_config/ucs_definitions.json`（UCS 类别定义），需要重新生成质心：

```bash
python tools/generate_platinum_centroids.py
```

**作用**:
- 从 `ucs_definitions.json` 生成 753 个 UCS CatID 的向量质心
- 用于 Level 2（AI 预测）

**输出**:
- `cache/platinum_centroids_754.pkl` - 质心文件

**何时需要**:
- 修改了 UCS 类别定义
- 添加了新的 CatID 描述
- 需要更新 AI 对类别的理解

---

#### 步骤 4: 快速验证（推荐）

使用微缩验证工具快速测试几个关键词：

```bash
# 验证几个大类
python tools/verify_subset.py AIR --limit 100
python tools/verify_subset.py WEAPON --limit 100
python tools/verify_subset.py VEHICLE --limit 100
```

**检查要点**:
1. **分类来源统计**：
   - Level -1（文件名短路）比例：如果文件名包含 CatID，应该被识别
   - Level 0（规则）比例是否提高？
   - Level 2（AI 预测）比例是否降低？
   - 如果 Level 0 比例太低，说明需要添加更多关键词映射

2. **主类别分布**：
   - 是否正确分类到预期的主类别（如 WEAPON, LASERS）？
   - 是否有大量 "UNCATEGORIZED"？
   - 如果有很多 "UNCATEGORIZED"，说明关键词映射不够

3. **聚类效果验证**（关键）：
   - 打开生成的 CSV 文件（`verify_{keyword}_details.csv`）
   - 按主类别排序，查看 UMAP_X 和 UMAP_Y 坐标
   - **同一主类别的数据应该在坐标上聚集**（X 和 Y 值相近）
   - 如果同一主类别的数据分散，说明聚类效果需要改进
   - 示例：所有 WEAPON 相关的数据应该在坐标 (5, 10) 附近聚集，而不是分散在 (1, 2), (8, 15), (3, 20) 等不同位置

4. **详细结果**：
   - 查看前20条数据的分类结果
   - 检查 rich_text 内容是否正确
   - 确认分类来源是否正确
   - 查看坐标分布，验证聚类效果

---

#### 步骤 5: 全量构建（验证通过后）

如果快速验证效果良好，运行全量构建：

```bash
python rebuild_atlas.py
```

**作用**:
- 重新向量化所有数据
- 重新计算 UMAP 坐标
- 应用新的分类规则

**耗时**: 10-30 分钟（取决于数据量）

---

### 完整工作流示例

假设你要添加新的关键词映射并验证 WEAPON 类别的聚类效果：

```bash
# 1. 编辑 CSV 文件（手动或使用 LLM）
# 在 data_config/ucs_alias.csv 中添加新行：
# laser gun, LASRGun
# magic fireball, MAGCFIRE
# sword hit, WEAPSwrd

# 2. 标准化 CSV（验证格式）
python tools/standardize_alias_csv.py

# 3. 生成规则文件
python tools/generate_rules_json.py

# 4. 快速验证（测试新添加的关键词）
python tools/verify_subset.py WEAPON --limit 100 --output verify_weapon.png

# 5. 检查验证结果：
#    - 查看控制台报告：分类来源统计、主类别分布
#    - 查看 PNG 图片：检查 WEAPON 相关的数据是否在坐标上聚集
#    - 打开 CSV 文件（verify_WEAPON_details.csv）：
#      * 按主类别排序
#      * 检查 UMAP_X 和 UMAP_Y 坐标
#      * 验证同一主类别的数据是否聚集（坐标相近）

# 6. 如果验证通过，运行全量构建
python rebuild_atlas.py
```

### UMAP 坐标说明

**UCS模式坐标**:
- **X轴/Y轴**: 基于固定坐标配置（`ucs_coordinates.json`）
- **坐标范围**: 0-3000（由固定坐标配置决定）
- **含义**: 大类位置固定，大类内部使用局部UMAP保持结构
- **特点**: 确保0%漂移，同一大类的数据强制在同一区域

**Gravity模式坐标**:
- **X轴（UMAP 维度 1）**: 降维后的第一个维度，表示数据在语义空间中的位置
- **Y轴（UMAP 维度 2）**: 降维后的第二个维度，表示数据在语义空间中的位置
- **坐标范围**: 归一化到 0-3000
- **含义**: 基于声学特征相似度的全局分布

**聚类效果验证**:
- **理想情况**: 同一主类别（如 WEAPON）的数据应该在坐标上聚集，形成"大陆"
- **验证方法**:
  1. 打开 CSV 文件，按主类别排序
  2. 查看同一主类别的 UMAP_X 和 UMAP_Y 值
  3. 计算坐标的方差或标准差
  4. 如果方差小，说明聚类效果好（数据聚集）
  5. 如果方差大，说明聚类效果差（数据分散）

**示例**:
```
主类别: WEAPON
- 文件1: CatID=WEAPSwrd, 坐标=(5.2, 10.1)
- 文件2: CatID=WEAPArmr, 坐标=(5.5, 10.3)
- 文件3: CatID=WEAPWhip, 坐标=(5.1, 9.8)
→ 坐标相近，聚类效果好 ✅

主类别: WEAPON
- 文件1: CatID=WEAPSwrd, 坐标=(1.2, 2.1)
- 文件2: CatID=WEAPArmr, 坐标=(8.5, 15.3)
- 文件3: CatID=WEAPWhip, 坐标=(3.1, 20.8)
→ 坐标分散，聚类效果差 ❌
```

---

### 数据维护检查清单

在更新数据后，使用以下清单确保所有步骤都完成：

- [ ] 更新 `data_config/ucs_alias.csv`（添加/修改关键词映射）
- [ ] 运行 `standardize_alias_csv.py` 验证格式
- [ ] 运行 `generate_rules_json.py` 生成规则文件
- [ ] （可选）更新 `ucs_definitions.json` 并重新生成质心
- [ ] （UCS模式）确保存在 `data_config/ucs_coordinates.json`（如不存在，运行 `tools/extract_category_centroids.py`）
- [ ] 使用 `verify_subset.py` 快速验证几个关键词（`--mode ucs` 或 `--mode gravity`）
- [ ] 检查验证报告，确认分类效果
- [ ] 如果效果良好，运行 `rebuild_atlas.py` 全量构建（包含UCS和Gravity模式）
- [ ] 启动软件，查看最终效果

---

## 注意事项

1. **缓存一致性**: 
   - 向量和坐标必须匹配
   - 如果只更新向量，必须重新计算坐标

2. **规则更新**:
   - 更新 `ucs_alias.csv` 后，**必须**运行 `generate_rules_json.py`
   - 否则分类系统仍会使用旧的规则

3. **验证流程**:
   - 建议先使用 `verify_subset.py` 快速验证
   - 确认效果后再运行全量构建
   - 可以节省大量时间

4. **内存占用**:
   - UMAP计算时，`n_neighbors=30` 比 `n_neighbors=15` 占用更多内存
   - 大数据集（>100万条）可能需要调整参数

5. **线程安全**:
   - 软件内的重建操作在后台线程执行
   - 不会阻塞UI

6. **进度反馈**:
   - 所有操作都有进度条显示
   - 关键步骤会输出日志

